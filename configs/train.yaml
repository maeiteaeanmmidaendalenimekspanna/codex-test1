seed: 1337
run_name: python7b-pretrain

model:
  n_layers: 32
  n_heads: 32
  hidden_size: 4096
  ffn_size: 11008
  max_seq_len: 8192
  vocab_size: 64000
  rope_theta: 10000

optimizer:
  type: adamw
  lr: 0.0003
  betas: [0.9, 0.95]
  weight_decay: 0.1
  grad_clip: 1.0

training:
  micro_batch_size: 2
  grad_accum_steps: 256
  total_steps: 500000
  log_every: 50
  save_every: 2000
  dtype: bf16
  device: cuda

data:
  train_shards: "data/shards/train/*.jsonl"
  val_shards: "data/shards/val/*.jsonl"
  pack_seq_len: 8192
  shuffle_buffer: 10000

output_dir: "runs/python7b"
