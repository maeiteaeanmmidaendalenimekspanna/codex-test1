# Data Pipeline (Python-Only)

## Data sources
- Public GitHub Python repositories.
- Python-related documentation and tutorials.
- Competitive programming Python solutions.
- Synthetic Python tasks generated by existing models (for diversity).

## Python extraction strategy
1. **Language detection**: only include files with Python extensions (`.py`, `.pyi`), plus Jupyter notebooks by extracting code cells.
2. **Quality filters**: remove minified or autogenerated files; enforce minimum line count and syntactic validity.
3. **Parser validation**: use `ast.parse` to ensure valid Python.
4. **Heuristic scoring**: prefer code with docstrings, tests, and modular structure.

## Deduplication
- **Exact dedup**: hashing normalized code.
- **Near-dup**: MinHash on token shingles.
- **Repository-level**: detect forks and exclude duplicates.

## Packing
- Pack examples to fixed length with document boundaries.
- Preserve file boundaries with a `<file_sep>` token to help code coherence.

## Output format
Each shard is a JSONL file:
```
{"id":"...","text":"<file_sep>...python code..."}
```
